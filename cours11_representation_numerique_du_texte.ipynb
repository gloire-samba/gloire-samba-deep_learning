{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "from sklearn.preprocessing import OneHotEncoder\n",
        "## Utilisation de keras\n",
        "from tensorflow.keras.preprocessing.text import Tokenizer\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "import numpy as np"
      ],
      "metadata": {
        "id": "YU1VMu7iih50"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# segmentation en mot"
      ],
      "metadata": {
        "id": "_KJe1kAVhIy-"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5MLa1q1sg5kT",
        "outputId": "5ab79ea4-57ad-40b8-acbb-92f18e632ac8"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['je', 'veux', 'apprendre', 'le', 'deep', 'learning']"
            ]
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "text = \" je veux apprendre le deep learning \"\n",
        "words = text.split()\n",
        "words"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# segmentation en charactère"
      ],
      "metadata": {
        "id": "jmTtu283hgpi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "character = list(text)\n",
        "print(character)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NckFAA6Nhf-k",
        "outputId": "62629b13-d153-43b2-b0bd-e3b6c6de9fd7"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[' ', 'j', 'e', ' ', 'v', 'e', 'u', 'x', ' ', 'a', 'p', 'p', 'r', 'e', 'n', 'd', 'r', 'e', ' ', 'l', 'e', ' ', 'd', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g', ' ']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Grame de mot"
      ],
      "metadata": {
        "id": "fx8eoyT6j5G-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Je veux apprendre le deep learning\"]\n",
        "vectorizer = CountVectorizer(ngram_range=(2, 2))\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(\"Bi-grams:\\n\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "abhZqHC9i4sZ",
        "outputId": "7b588e12-cd4f-49f0-e3c1-52f9fcc434fa"
      },
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-grams:\n",
            " ['apprendre le' 'deep learning' 'je veux' 'le deep' 'veux apprendre']\n",
            "[[1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Je veux apprendre le deep learning\"]\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 2))\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(\"Bi-grams:\\n\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KCpUPdXhi7Tw",
        "outputId": "c2596bb3-81e5-4638-a6ca-5a141b3b1686"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-grams:\n",
            " ['apprendre' 'apprendre le' 'deep' 'deep learning' 'je' 'je veux' 'le'\n",
            " 'le deep' 'learning' 'veux' 'veux apprendre']\n",
            "[[1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "text = [\"Je veux apprendre le deep learning\"]\n",
        "vectorizer = CountVectorizer(ngram_range=(1, 3))\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(\"Bi-grams:\\n\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KscU0YZBieza",
        "outputId": "63eef700-9d27-4578-cf32-339986d0a4c6"
      },
      "execution_count": 27,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-grams:\n",
            " ['apprendre' 'apprendre le' 'apprendre le deep' 'deep' 'deep learning'\n",
            " 'je' 'je veux' 'je veux apprendre' 'le' 'le deep' 'le deep learning'\n",
            " 'learning' 'veux' 'veux apprendre' 'veux apprendre le']\n",
            "[[1 1 1 1 1 1 1 1 1 1 1 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# N-Grame de charactère"
      ],
      "metadata": {
        "id": "U2HUma79j9iq"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "vectorizer = CountVectorizer(analyzer='char', ngram_range=(2, 2))\n",
        "X = vectorizer.fit_transform(text)\n",
        "print(\"Bi-grams de caractères:\\n\", vectorizer.get_feature_names_out())\n",
        "print(X.toarray())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BV9WWw-dkcTA",
        "outputId": "e5ccf236-81de-4c79-f612-c8ab80e7ef74"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Bi-grams de caractères:\n",
            " [' a' ' d' ' l' ' v' 'ap' 'ar' 'de' 'dr' 'e ' 'ea' 'ee' 'en' 'ep' 'eu'\n",
            " 'in' 'je' 'le' 'nd' 'ng' 'ni' 'p ' 'pp' 'pr' 're' 'rn' 'ux' 've' 'x ']\n",
            "[[1 1 2 1 1 1 1 1 3 1 1 1 1 1 1 1 2 1 1 1 1 1 1 2 1 1 1 1]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# One hot encoding"
      ],
      "metadata": {
        "id": "KfZXfUvToM1S"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one hot encoding sklearn"
      ],
      "metadata": {
        "id": "zGw4bjdfoQ83"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texte d'exemple à analyser\n",
        "text = [\"Je veux apprendre le deep learning\"]\n",
        "\n",
        "# Séparation du texte en mots\n",
        "words = text[0].split()\n",
        "print(\"Mots:\", words)\n",
        "\n",
        "# Initialisation de l'encodeur one-hot\n",
        "encoder = OneHotEncoder(sparse_output=False)\n",
        "\n",
        "# Transformation des mots en vecteurs one-hot\n",
        "words_array = np.array(words).reshape(-1, 1)\n",
        "one_hot_encoded_words = encoder.fit_transform(words_array)\n",
        "print(\"One-hot Encoding des Mots:\\n\", one_hot_encoded_words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "45nWl6mvlieu",
        "outputId": "4b6c47ab-4f17-4865-aeca-f38704b1d6e6"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mots: ['Je', 'veux', 'apprendre', 'le', 'deep', 'learning']\n",
            "One-hot Encoding des Mots:\n",
            " [[1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1.]\n",
            " [0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0.]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## one hot encoding tensorflow"
      ],
      "metadata": {
        "id": "J2nD3ug8oXMQ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### avec des mots"
      ],
      "metadata": {
        "id": "fuWjK77UoaoR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Texte d'exemple à analyser\n",
        "text = [\"Je veux apprendre le deep learning\"]\n",
        "\n",
        "# Initialisation du tokenizer\n",
        "tokenizer = Tokenizer()\n",
        "tokenizer.fit_on_texts(text)\n",
        "\n",
        "# Transformation des mots en séquences d'indices\n",
        "sequences = tokenizer.texts_to_sequences(text)\n",
        "print(\"Sequences:\", sequences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wxnaR1T9m0J-",
        "outputId": "58c95abb-a094-4dda-f8df-e6ba0db67f7d"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sequences: [[1, 2, 3, 4, 5, 6]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Transformation en one-hot encoding\n",
        "word_index = tokenizer.word_index\n",
        "one_hot_encoded_words = to_categorical(sequences)\n",
        "print(\"One-hot Encoding des Mots avec Keras:\\n\", one_hot_encoded_words)\n",
        "print(\"Word Index:\", word_index)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ML2FpGzWnS9T",
        "outputId": "daf17905-46e8-47e0-d600-318215d53717"
      },
      "execution_count": 31,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "One-hot Encoding des Mots avec Keras:\n",
            " [[[0. 1. 0. 0. 0. 0. 0.]\n",
            "  [0. 0. 1. 0. 0. 0. 0.]\n",
            "  [0. 0. 0. 1. 0. 0. 0.]\n",
            "  [0. 0. 0. 0. 1. 0. 0.]\n",
            "  [0. 0. 0. 0. 0. 1. 0.]\n",
            "  [0. 0. 0. 0. 0. 0. 1.]]]\n",
            "Word Index: {'je': 1, 'veux': 2, 'apprendre': 3, 'le': 4, 'deep': 5, 'learning': 6}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### avec des charactères"
      ],
      "metadata": {
        "id": "Xb9YKUYgoeVC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Séparation du texte en caractères\n",
        "characters = list(text[0])\n",
        "print(\"Caractères:\", characters)\n",
        "\n",
        "# Transformation des caractères en vecteurs one-hot\n",
        "characters_array = np.array(characters).reshape(-1, 1)\n",
        "one_hot_encoded_characters = encoder.fit_transform(characters_array)\n",
        "print(\"One-hot Encoding des Caractères:\\n\", one_hot_encoded_characters)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MOA3Yjz6ojeF",
        "outputId": "4e900583-c138-4731-b37d-1b5deeefa2e3"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Caractères: ['J', 'e', ' ', 'v', 'e', 'u', 'x', ' ', 'a', 'p', 'p', 'r', 'e', 'n', 'd', 'r', 'e', ' ', 'l', 'e', ' ', 'd', 'e', 'e', 'p', ' ', 'l', 'e', 'a', 'r', 'n', 'i', 'n', 'g']\n",
            "One-hot Encoding des Caractères:\n",
            " [[0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0.]\n",
            " [1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n",
            " [0. 0. 0. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 0.]]\n"
          ]
        }
      ]
    }
  ]
}